{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "# HIDDEN CELL\n",
    "import sys, os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Importing argopy in dev mode:\n",
    "on_rtd = os.environ.get('READTHEDOCS', None) == 'True'\n",
    "if not on_rtd:\n",
    "    sys.path.insert(0, \"/Users/gmaze/git/github/euroargodev/argopy\")\n",
    "    import git\n",
    "    import argopy\n",
    "    from argopy.options import OPTIONS\n",
    "    print(\"argopy:\", argopy.__version__, \n",
    "          \"\\nsrc:\", argopy.__file__, \n",
    "          \"\\nbranch:\", git.Repo(search_parent_directories=True).active_branch.name, \n",
    "          \"\\noptions:\", OPTIONS)\n",
    "else:\n",
    "    sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "import xarray as xr\n",
    "# xr.set_options(display_style=\"html\");\n",
    "xr.set_options(display_style=\"text\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argopy\n",
    "from argopy import DataFetcher as ArgoDataFetcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "If you want to avoid retrieving the same data several times during a working session, or if you fetched a large amount of data, you may want to temporarily save data in a cache file.\n",
    "\n",
    "You can cache fetched data with the fetchers option ``cache``.\n",
    "\n",
    "**Argopy** cached data are persistent, meaning that they are stored locally on files and will survive execution of your script with a new session. \n",
    "They have however an expiration time of one day, since this is the update frequency of most data sources. This will ensure you always have the last version of Argo data.\n",
    "\n",
    "All data and meta-data (index) fetchers have a caching system.\n",
    "\n",
    "The argopy default cache folder is under your home directory at ``~./.cache/argopy``. \n",
    "But you can specify the path you want to use in several ways:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "- with **argopy** global options:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "```python\n",
    "argopy.set_options(cachedir='mycache_folder')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- in a temporary context:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "with argopy.set_options(cachedir='mycache_folder'):\n",
    "    ds = ArgoDataFetcher(cache=True).profile(6902746, 34).to_xarray()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- when instantiating the data fetcher:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "ds = ArgoDataFetcher(cache=True, cachedir='mycache_folder').profile(6902746, 34).to_xarray()\n",
    "```"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    ".. warning::\n",
    "\n",
    "  You really need to set the ``cache`` option to ``True``. Specifying only the ``cachedir`` won't trigger caching !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifyng a cache directory at the fetcher level will ensure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clearing the cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cached data have an expiration time of 1 day.\n",
    "\n",
    "If you want to manuallt clear your cache folder, and/or make sure your data are newly fetched, you can do it at the fetcher level with the ``clear_cache`` method.\n",
    "\n",
    "Start to fetch data and store them in cache:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "fetcher = ArgoDataFetcher(cache=True, cachedir='mycache_folder').profile(6902746, 34)\n",
    "fetcher.to_xarray();\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetched data are in the local cache folder:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "os.listdir('mycache_folder')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where we see one hash entries the newly fetched data and the cache registry file ``cache``.\n",
    "\n",
    "We can then fetch something else:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "fetcher2 = ArgoDataFetcher(cache=True, cachedir='mycache_folder').profile(1901393, 1)\n",
    "fetcher2.to_xarray();\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All fetched data are now cached in 'mycache_folder':"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "os.listdir('mycache_folder')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the new hash file from the ``fetcher2`` data.\n",
    "\n",
    "We can safely clear the cache from the first fetcher data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "fetcher.clear_cache()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "os.listdir('mycache_folder')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using the fetcher level clear cache, you make sure that only data fetched with it are removed, while other fetched data (with other fetchers for instance) will stay in place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to clear the entire cache folder, whatever the fetcher used, do it at the package level with:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "argopy.clear_cache()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "os.listdir('mycache_folder')\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "FileNotFoundError                         Traceback (most recent call last)\n",
    "<ipython-input-13-6726e674f21f> in <module>\n",
    "----> 1 os.listdir('mycache_folder')\n",
    "\n",
    "FileNotFoundError: [Errno 2] No such file or directory: 'mycache_folder'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel fetching\n",
    "\n",
    "Sometimes you may find that your request takes a long time to fetch, or simply does not even succeed. You can then try to let argopy chunks your request into smaller pieces and have it fetched in parallel for you. This is done with the argument ``parallel`` of the data fetcher and can be tuned using options ``chunks`` and ``chunksize``.\n",
    "\n",
    "This goes by default like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the box to load (large enough to trigger chunking):\n",
    "box = [-60, -30, 40.0, 60.0, 0.0, 100.0, \"2007-01-01\", \"2007-04-01\"]\n",
    "\n",
    "# Instantiate a parallel fetcher:\n",
    "loader_par = ArgoDataFetcher(src='erddap', parallel=True).region(box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can also use the option ``progress`` to display a progress bar during fetching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_par = ArgoDataFetcher(src='erddap', parallel=True, progress=True).region(box)\n",
    "loader_par"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you can fetch data as usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds = loader_par.to_xarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**chunks**\n",
    "\n",
    "To check how many chunks your request has been split into, you can look at the ``uri`` property of the fetcher, it gives the list of paths toward data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_par.uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To control chunking, you can use the ``chunks`` option that specify the number of chunks in each of the *direction*, i.e. ``lon``, ``lat``, ``dpt`` and ``time`` for a *box* fetching and ``wmo`` for a *float* or *profile* fetching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a large box:\n",
    "box = [-60, -10, 40.0, 60.0 + np.random.randint(0,100,1)[0]/1000, 0.0, 500.0, \"2007\", \"2010\"]\n",
    "\n",
    "# Init parallel fetcher:\n",
    "loader_par = ArgoDataFetcher(src='erddap', parallel=True, \n",
    "                             chunks={'lon': 5}).region(box)\n",
    "# Number of chunks:\n",
    "len(loader_par.uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates 130 chunks, and 5 along the longitudinale direction, as requested. When the ``chunks`` option is not specified for a given *direction*, it relies on auto-chunking using pre-defined chunk maximum sizes. In the case above, this explains why we have 130 and not only 5 chunks.\n",
    "To chunk the requested along a single direction, set all the others to ``1``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_par = ArgoDataFetcher(src='argovis', parallel=True, \n",
    "                             chunks={'lon': 5, 'lat':1, 'dpt':1, 'time':1}).region(box)\n",
    "len(loader_par.uri)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    ".. note::\n",
    "    If you run the last command with the `argovis` fetcher, you will still have more than 5 chunks (i.e. 65) because `argovis` is limited to 3 months length requests, so we still have to split this multi-year request.\n",
    "    \n",
    ".. warning::\n",
    "    The `localftp` fetcher and the `float` and `profile` access points of `argovis` uses a list of resources than are not chunked but fetched in parallel using a batch queue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of performances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare performance with or without the parallel option, we need to make sure data are not cached on the server side.\n",
    "To do this, we use a very small random perturbation on the box definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "box = [-60, -10, 40.0, 60.0 + np.random.randint(0,100,1)[0]/1000, 0.0, 500.0, \"2007\", \"2010\"]\n",
    "print(box)\n",
    "ds = ArgoDataFetcher(src='argovis', parallel=False).region(box).to_xarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "box = [-60, 0, 40.0, 60.0 + np.random.randint(0,100,1)[0]/1000, 0.0, 500.0, \"2007\",\"2010\"]\n",
    "print(box)\n",
    "ds = ArgoDataFetcher(src='argovis', parallel=True, progress=True).region(box).to_xarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**results**: This simple comparison shows that parallel requests are usefull to handle very large region of data but that it provides a significant overhead for regular size requests."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
