{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "argopy: 0.1.5 \n",
      "src: /Users/gmaze/git/github/euroargodev/argopy/argopy/__init__.py \n",
      "branch: parallel-requests \n",
      "options: {'src': 'erddap', 'local_ftp': '.', 'dataset': 'phy', 'cachedir': '/Users/gmaze/.cache/argopy', 'mode': 'standard', 'api_timeout': 60}\n"
     ]
    }
   ],
   "source": [
    "# HIDDEN CELL\n",
    "import sys, os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Importing argopy in dev mode:\n",
    "on_rtd = os.environ.get('READTHEDOCS', None) == 'True'\n",
    "if not on_rtd:\n",
    "    sys.path.insert(0, \"/Users/gmaze/git/github/euroargodev/argopy\")\n",
    "    import git\n",
    "    import argopy\n",
    "    from argopy.options import OPTIONS\n",
    "    print(\"argopy:\", argopy.__version__, \n",
    "          \"\\nsrc:\", argopy.__file__, \n",
    "          \"\\nbranch:\", git.Repo(search_parent_directories=True).active_branch.name, \n",
    "          \"\\noptions:\", OPTIONS)\n",
    "else:\n",
    "    sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "import xarray as xr\n",
    "# xr.set_options(display_style=\"html\");\n",
    "xr.set_options(display_style=\"text\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argopy\n",
    "from argopy import DataFetcher as ArgoDataFetcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve ``argopy`` data fetching performances (in terms of time of retrieval), 2 solutions are available:\n",
    "    \n",
    "- Cache fetched data, i.e. save your request locally so that you don't have to fetch it again,\n",
    "- Fetch data by chunks in parallel, i.e. fetch peace of independant data simultaneously.\n",
    "\n",
    "These solutions are explained below.\n",
    "\n",
    "Note that another solution from standard big data strategies would be to fetch data lazily. But since (i) *argopy* post-processes raw Argo data on the client side and (ii) none of the data sources are cloud/lazy compatible, this solution is not possible (yet)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "If you want to avoid retrieving the same data several times during a working session, or if you fetched a large amount of data, you may want to temporarily save data in a cache file.\n",
    "\n",
    "You can cache fetched data with the fetchers option ``cache``.\n",
    "\n",
    "**Argopy** cached data are persistent, meaning that they are stored locally on files and will survive execution of your script with a new session. \n",
    "**Cached data have an expiration time of one day**, since this is the update frequency of most data sources. This will ensure you always have the last version of Argo data.\n",
    "\n",
    "All data and meta-data (index) fetchers have a caching system.\n",
    "\n",
    "The argopy default cache folder is under your home directory at ``~/.cache/argopy``. \n",
    "\n",
    "But you can specify the path you want to use in several ways:\n",
    "\n",
    "- with **argopy** global options:\n",
    "\n",
    "```python\n",
    "argopy.set_options(cachedir='mycache_folder')\n",
    "```\n",
    "\n",
    "- in a temporary context:\n",
    "\n",
    "```python\n",
    "with argopy.set_options(cachedir='mycache_folder'):\n",
    "    ds = ArgoDataFetcher(cache=True).profile(6902746, 34).to_xarray()\n",
    "```\n",
    "\n",
    "- when instantiating the data fetcher:\n",
    "\n",
    "```python\n",
    "ds = ArgoDataFetcher(cache=True, cachedir='mycache_folder').profile(6902746, 34).to_xarray()\n",
    "```"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    ".. warning::\n",
    "\n",
    "  You really need to set the ``cache`` option to ``True``. Specifying only the ``cachedir`` won't trigger caching !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clearing the cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to manually clear your cache folder, and/or make sure your data are newly fetched, you can do it at the fetcher level with the ``clear_cache`` method.\n",
    "\n",
    "Start to fetch data and store them in cache:\n",
    "\n",
    "```python\n",
    "fetcher = ArgoDataFetcher(cache=True, cachedir='mycache_folder').profile(6902746, 34)\n",
    "fetcher.to_xarray();\n",
    "```\n",
    "\n",
    "Fetched data are in the local cache folder:\n",
    "```python\n",
    "os.listdir('mycache_folder')\n",
    "```\n",
    "```bash\n",
    "['cache', \n",
    " 'c5c820b6aff7b2ef86ef00626782587a95d37edc54120a63ee4699be2b0c6b7c']\n",
    "```\n",
    "\n",
    "where we see one hash entries the newly fetched data and the cache registry file ``cache``.\n",
    "\n",
    "We can then fetch something else using the same cache folder:\n",
    "\n",
    "```python\n",
    "fetcher2 = ArgoDataFetcher(cache=True, cachedir='mycache_folder').profile(1901393, 1)\n",
    "fetcher2.to_xarray();\n",
    "```\n",
    "\n",
    "All fetched data are cached:\n",
    "\n",
    "```python\n",
    "os.listdir('mycache_folder')\n",
    "```\n",
    "```bash\n",
    "['cache',\n",
    " 'c5c820b6aff7b2ef86ef00626782587a95d37edc54120a63ee4699be2b0c6b7c',\n",
    " '58072df8477157c194449a2e6dff8d69ca3c8fded01eebdd8a5fc446f2f7f9a7']\n",
    "```\n",
    "\n",
    "Note the new hash file with the ``fetcher2`` data.\n",
    "\n",
    "It is important to note that we can safely clear the cache from the first ``fetcher`` data, it won't remove the ``fetcher2`` data:\n",
    "\n",
    "```python\n",
    "fetcher.clear_cache()\n",
    "os.listdir('mycache_folder')\n",
    "```\n",
    "```bash\n",
    "['cache', \n",
    " '58072df8477157c194449a2e6dff8d69ca3c8fded01eebdd8a5fc446f2f7f9a7']\n",
    "```\n",
    "\n",
    "By using the fetcher level clear cache, you make sure that only data fetched with it are removed, while other fetched data (with other fetchers for instance) will stay in place.\n",
    "\n",
    "If you want to clear the entire cache folder, whatever the fetcher used, do it at the package level with:\n",
    "\n",
    "```python\n",
    "argopy.clear_cache()\n",
    "```\n",
    "\n",
    "So, if we now check the cache folder, it's been deleted:\n",
    "\n",
    "```python\n",
    "os.listdir('mycache_folder')\n",
    "```\n",
    "```bash\n",
    "---------------------------------------------------------------------------\n",
    "FileNotFoundError                         Traceback (most recent call last)\n",
    "<ipython-input-13-6726e674f21f> in <module>\n",
    "----> 1 os.listdir('mycache_folder')\n",
    "\n",
    "FileNotFoundError: [Errno 2] No such file or directory: 'mycache_folder'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel data fetching\n",
    "\n",
    "Sometimes you may find that your request takes a long time to fetch, or simply does not even succeed. This is probably because you're trying to fetch a large amount of data.\n",
    "\n",
    "In this case, you can try to let argopy chunks your request into smaller pieces and have them fetched in parallel for you. This is done with the argument ``parallel`` of the data fetcher and can be tuned using options ``chunks`` and ``chunksize``.\n",
    "\n",
    "This goes by default like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a box to load (large enough to trigger chunking):\n",
    "box = [-60, -30, 40.0, 60.0, 0.0, 100.0, \"2007-01-01\", \"2007-04-01\"]\n",
    "\n",
    "# Instantiate a parallel fetcher:\n",
    "loader_par = ArgoDataFetcher(src='erddap', parallel=True).region(box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can also use the option ``progress`` to display a progress bar during fetching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<datafetcher.erddap>\n",
       "Name: Ifremer erddap Argo data fetcher for a space/time region\n",
       "API: https://www.ifremer.fr/erddap\n",
       "Domain: phy_[x=-60.00/-30.00; y=40.00/ ... 00.0; t=2007-01-01/2007-04-01]\n",
       "Backend: erddap (parallel=True)\n",
       "User mode: standard"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader_par = ArgoDataFetcher(src='erddap', parallel=True, progress=True).region(box)\n",
    "loader_par"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you can fetch data as usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  3.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 761 ms, sys: 114 ms, total: 874 ms\n",
      "Wall time: 889 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ds = loader_par.to_xarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check how many chunks your request has been split into, you can look at the ``uri`` property of the fetcher, it gives you the list of paths toward data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http: ...  longitude>=-60.0&longitude<=-45.0&latitude>=40.0&latitude<=60.0&pres>=0.0&pres<=100.0&time>=1167609600.0&time<=1175385600.0\n",
      "http: ...  longitude>=-45.0&longitude<=-30.0&latitude>=40.0&latitude<=60.0&pres>=0.0&pres<=100.0&time>=1167609600.0&time<=1175385600.0\n"
     ]
    }
   ],
   "source": [
    "# Display only the relevant part of each URLs of URI:\n",
    "for uri in loader_par.uri:\n",
    "    print(\"http: ... \", \"&\".join(uri.split(\"&\")[1:-2])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To control chunking, you can use the **``chunks``** option that specifies the number of chunks in each of the *direction*:\n",
    "\n",
    "- ``lon``, ``lat``, ``dpt`` and ``time`` for a **region** fetching,\n",
    "- ``wmo`` for a **float** and **profile** fetching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "195"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a large box:\n",
    "box = [-60, 0, 0.0, 60.0, 0.0, 500.0, \"2007\", \"2010\"]\n",
    "\n",
    "# Init a parallel fetcher:\n",
    "loader_par = ArgoDataFetcher(src='erddap', \n",
    "                             parallel=True, \n",
    "                             chunks={'lon': 5}).region(box)\n",
    "# Check number of chunks:\n",
    "len(loader_par.uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates 195 chunks, and 5 along the longitudinale direction, as requested. \n",
    "\n",
    "When the ``chunks`` option is not specified for a given *direction*, it relies on auto-chunking using pre-defined chunk maximum sizes (see below). \n",
    "In the case above, this explains why we have 195 and not only 5 chunks.\n",
    "\n",
    "To chunk the request along a single direction, set explicitely all the others direction to ``1``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Init a parallel fetcher:\n",
    "loader_par = ArgoDataFetcher(src='erddap', \n",
    "                             parallel=True, \n",
    "                             chunks={'lon': 5, 'lat':1, 'dpt':1, 'time':1}).region(box)\n",
    "\n",
    "# Check number of chunks:\n",
    "len(loader_par.uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have 5 chunks along longitude, check out the URLs parameter in the list of URIs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longitude>=-60.0&longitude<=-48.0&latitude>=0.0&latitude<=60.0&pres>=0.0&pres<=500.0&time>=1167609600.0&time<=1262304000.0\n",
      "longitude>=-48.0&longitude<=-36.0&latitude>=0.0&latitude<=60.0&pres>=0.0&pres<=500.0&time>=1167609600.0&time<=1262304000.0\n",
      "longitude>=-36.0&longitude<=-24.0&latitude>=0.0&latitude<=60.0&pres>=0.0&pres<=500.0&time>=1167609600.0&time<=1262304000.0\n",
      "longitude>=-24.0&longitude<=-12.0&latitude>=0.0&latitude<=60.0&pres>=0.0&pres<=500.0&time>=1167609600.0&time<=1262304000.0\n",
      "longitude>=-12.0&longitude<=0.0&latitude>=0.0&latitude<=60.0&pres>=0.0&pres<=500.0&time>=1167609600.0&time<=1262304000.0\n"
     ]
    }
   ],
   "source": [
    "for uri in loader_par.uri:\n",
    "    print(\"&\".join(uri.split(\"&\")[1:-2])) # Display only the relevant URL part"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    ".. note::\n",
    "    You may notice that if you run the last command with the `argovis` fetcher, you will still have more than 5 chunks (i.e. 65). This is because `argovis` is limited to 3 months length requests. So, for this requet that is 3 years long, argopy ends up with 13 chunks along time, times 5 chunks in longitude, leading to 65 chunks in total."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    ".. warning::\n",
    "    The `localftp` fetcher and the `float` and `profile` access points of the `argovis` fetcher use a list of resources than are not chunked but fetched in parallel using a batch queue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Size of chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default chunk size for each access point dimensions are:\n",
    "\n",
    "| Access point dimension | Maximum chunk size |\n",
    "|------------------------|:------------------:|\n",
    "| region / **lon**       |       20 deg       |\n",
    "| region / **lat**       |       20 deg       |\n",
    "| region / **dpt**       |      500 m or db   |\n",
    "| region / **time**      |       90 days      |\n",
    "| float / **wmo**        |          5         |\n",
    "| profile / **wmo**      |          5         |\n",
    "\n",
    "These default values are used to chunk data where the ``chunks`` parameter key is set to ``auto``.\n",
    "\n",
    "But you can modify the maximum chunk size allowed in each of the possible directions. This is done with the option **``chunks_maxsize``**.\n",
    "\n",
    "For instance if you want to make sure that your chunks are not larger then 100 meters (db) in depth (pressure), you can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "195"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a large box:\n",
    "box = [-60, -10, 40.0, 60.0, 0.0, 500.0, \"2007\", \"2010\"]\n",
    "\n",
    "# Init a parallel fetcher:\n",
    "loader_par = ArgoDataFetcher(src='erddap', \n",
    "                             parallel=True, \n",
    "                             chunks_maxsize={'dpt': 100}).region(box)\n",
    "# Check number of chunks:\n",
    "len(loader_par.uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this creates a large number of chunks, let's do this again and combine with the option ``chunks`` to see easily what's going on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http: ...  longitude>=-60&longitude<=-10&latitude>=40.0&latitude<=60.0&pres>=0.0&pres<=100.0&time>=1167609600.0&time<=1262304000.0\n",
      "http: ...  longitude>=-60&longitude<=-10&latitude>=40.0&latitude<=60.0&pres>=100.0&pres<=200.0&time>=1167609600.0&time<=1262304000.0\n",
      "http: ...  longitude>=-60&longitude<=-10&latitude>=40.0&latitude<=60.0&pres>=200.0&pres<=300.0&time>=1167609600.0&time<=1262304000.0\n",
      "http: ...  longitude>=-60&longitude<=-10&latitude>=40.0&latitude<=60.0&pres>=300.0&pres<=400.0&time>=1167609600.0&time<=1262304000.0\n",
      "http: ...  longitude>=-60&longitude<=-10&latitude>=40.0&latitude<=60.0&pres>=400.0&pres<=500.0&time>=1167609600.0&time<=1262304000.0\n"
     ]
    }
   ],
   "source": [
    "# Init a parallel fetcher with chunking along the vertical axis alone:\n",
    "loader_par = ArgoDataFetcher(src='erddap', \n",
    "                             parallel=True, \n",
    "                             chunks_maxsize={'dpt': 100},\n",
    "                             chunks={'lon':1, 'lat':1, 'dpt':'auto', 'time':1}).region(box)\n",
    "\n",
    "for uri in loader_par.uri:\n",
    "    print(\"http: ... \", \"&\".join(uri.split(\"&\")[1:-2])) # Display only the relevant URL part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see, that the ``pres`` argument of this erddap list of URLs define layers not thicker than the requested 100db."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the ``profile`` and ``float`` access points, you can use the ``wmo`` keyword to control the number of WMOs in each chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http: ...  platform_number=~\"6902766|6902772|6902914\"\n",
      "http: ...  platform_number=~\"6902746|6902916|6902915\"\n",
      "http: ...  platform_number=~\"6902757|6902771\"\n"
     ]
    }
   ],
   "source": [
    "WMO_list = [6902766, 6902772, 6902914, 6902746, 6902916, 6902915, 6902757, 6902771]\n",
    "\n",
    "# Init a parallel fetcher with chunking along the list of WMOs:\n",
    "loader_par = ArgoDataFetcher(src='erddap', \n",
    "                             parallel=True, \n",
    "                             chunks_maxsize={'wmo': 3}).float(WMO_list)\n",
    "\n",
    "for uri in loader_par.uri:\n",
    "    print(\"http: ... \", \"&\".join(uri.split(\"&\")[1:-2])) # Display only the relevant URL part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see here, that this request for 8 floats is split in chunks with no more that 3 floats each."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    ".. note::\n",
    "    At this point, there is no mechanism to chunk requests along cycle numbers for the ``profile`` access point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelization methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are 2 methods available to set-up your data fetching requests in parallel:\n",
    "    \n",
    "1. [Multi-threading](https://en.wikipedia.org/wiki/Multithreading_(computer_architecture))\n",
    "1. [Multi-processing](https://en.wikipedia.org/wiki/Multiprocessing)\n",
    "\n",
    "Both options use a pool of [threads](https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ThreadPoolExecutor) or [processes](https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ProcessPoolExecutor) managed with the [concurrent futures module](https://docs.python.org/3/library/concurrent.futures.html#module-concurrent.futures). \n",
    "\n",
    "The parallelization method is set with the ``parallel_method`` option of the fetcher, which can take as values ``thread`` or ``process``.\n",
    "\n",
    "We recall that:\n",
    "\n",
    "| **Parallel method**     | erddap | localftp | argovis |\n",
    "|-------------------------|:------:|:--------:|:-------:|\n",
    "| Multi-threading         |    X   |     X    |    X    |\n",
    "| Multi-processes         |        |     X    |         |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOX= [-60, -30, 40.0, 60.0, 0.0, 100.0, '2007-01-01', '2007-04-01']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:02<00:00,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 193 ms, sys: 13.7 ms, total: 206 ms\n",
      "Wall time: 2.19 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "box = [-60, 0, \n",
    "       20.0, 60.0 + np.random.randint(0,100,1)[0]/1000, \n",
    "       0.0, 500.0, \n",
    "       \"2007\", \"2010\"]\n",
    "box = [-60, -30, 40.0, 60.0, 0.0, 100.0, \"2007-01-01\", \"2007-04-01\"]\n",
    "\n",
    "print(\"BOX=\", box)\n",
    "ds = ArgoDataFetcher(src='argovis', parallel=True, progress=True).region(box).to_xarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of performances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that to compare performances with or without the parallel option, we need to make sure that data are not cached on the server side.\n",
    "To do this, we use a very small random perturbation on the box definition, here on the maximum latitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def this_box():\n",
    "    return [-60, 0, \n",
    "           20.0, 60.0 + np.random.randint(0,100,1)[0]/1000, \n",
    "           0.0, 500.0, \n",
    "           \"2007\", \"2010\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.8 s, sys: 1.56 s, total: 12.4 s\n",
      "Wall time: 1min 1s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>&lt;xarray.Dataset&gt;\n",
       "Dimensions:          (N_POINTS: 976111)\n",
       "Coordinates:\n",
       "  * N_POINTS         (N_POINTS) int64 0 1 2 3 4 ... 976107 976108 976109 976110\n",
       "    LATITUDE         (N_POINTS) float64 40.22 40.22 40.22 ... 45.74 45.74 45.74\n",
       "    TIME             (N_POINTS) object &#x27;2007-01-01T01:05:33.792Z&#x27; ... &#x27;2009-12-31T23:33:00.000Z&#x27;\n",
       "    LONGITUDE        (N_POINTS) float64 -20.03 -20.03 -20.03 ... -45.62 -45.62\n",
       "Data variables:\n",
       "    CYCLE_NUMBER     (N_POINTS) int64 38 38 38 38 38 38 38 ... 40 40 40 40 40 40\n",
       "    DATA_MODE        (N_POINTS) &lt;U1 &#x27;D&#x27; &#x27;D&#x27; &#x27;D&#x27; &#x27;D&#x27; &#x27;D&#x27; ... &#x27;D&#x27; &#x27;D&#x27; &#x27;D&#x27; &#x27;D&#x27; &#x27;D&#x27;\n",
       "    DIRECTION        (N_POINTS) &lt;U1 &#x27;A&#x27; &#x27;A&#x27; &#x27;A&#x27; &#x27;A&#x27; &#x27;A&#x27; ... &#x27;A&#x27; &#x27;A&#x27; &#x27;A&#x27; &#x27;A&#x27; &#x27;A&#x27;\n",
       "    PLATFORM_NUMBER  (N_POINTS) int64 6900256 6900256 ... 4901103 4901103\n",
       "    POSITION_QC      (N_POINTS) int64 1 1 1 1 1 1 1 1 1 1 ... 1 1 1 1 1 1 1 1 1\n",
       "    PRES             (N_POINTS) float64 4.5 8.8 18.5 28.6 ... 399.4 449.4 499.0\n",
       "    PSAL             (N_POINTS) float64 36.05 36.05 36.05 ... 35.04 34.9 34.89\n",
       "    TEMP             (N_POINTS) float64 15.73 15.73 15.73 ... 6.415 5.196 4.877\n",
       "    TIME_QC          (N_POINTS) int64 1 1 1 1 1 1 1 1 1 1 ... 1 1 1 1 1 1 1 1 1\n",
       "Attributes:\n",
       "    DATA_ID:              ARGO\n",
       "    DOI:                  http://doi.org/10.17882/42182\n",
       "    Fetched_from:         https://argovis.colorado.edu\n",
       "    Fetched_by:           gmaze\n",
       "    Fetched_date:         2020/08/26\n",
       "    Fetched_constraints:  phy_[x=-60.00/0.00; y=20.00/60.07; z=0.0/500.0; t=2...\n",
       "    Fetched_uri:          [&#x27;https://argovis.colorado.edu/selection/profiles?s...</pre>"
      ],
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:          (N_POINTS: 976111)\n",
       "Coordinates:\n",
       "  * N_POINTS         (N_POINTS) int64 0 1 2 3 4 ... 976107 976108 976109 976110\n",
       "    LATITUDE         (N_POINTS) float64 40.22 40.22 40.22 ... 45.74 45.74 45.74\n",
       "    TIME             (N_POINTS) object '2007-01-01T01:05:33.792Z' ... '2009-12-31T23:33:00.000Z'\n",
       "    LONGITUDE        (N_POINTS) float64 -20.03 -20.03 -20.03 ... -45.62 -45.62\n",
       "Data variables:\n",
       "    CYCLE_NUMBER     (N_POINTS) int64 38 38 38 38 38 38 38 ... 40 40 40 40 40 40\n",
       "    DATA_MODE        (N_POINTS) <U1 'D' 'D' 'D' 'D' 'D' ... 'D' 'D' 'D' 'D' 'D'\n",
       "    DIRECTION        (N_POINTS) <U1 'A' 'A' 'A' 'A' 'A' ... 'A' 'A' 'A' 'A' 'A'\n",
       "    PLATFORM_NUMBER  (N_POINTS) int64 6900256 6900256 ... 4901103 4901103\n",
       "    POSITION_QC      (N_POINTS) int64 1 1 1 1 1 1 1 1 1 1 ... 1 1 1 1 1 1 1 1 1\n",
       "    PRES             (N_POINTS) float64 4.5 8.8 18.5 28.6 ... 399.4 449.4 499.0\n",
       "    PSAL             (N_POINTS) float64 36.05 36.05 36.05 ... 35.04 34.9 34.89\n",
       "    TEMP             (N_POINTS) float64 15.73 15.73 15.73 ... 6.415 5.196 4.877\n",
       "    TIME_QC          (N_POINTS) int64 1 1 1 1 1 1 1 1 1 1 ... 1 1 1 1 1 1 1 1 1\n",
       "Attributes:\n",
       "    DATA_ID:              ARGO\n",
       "    DOI:                  http://doi.org/10.17882/42182\n",
       "    Fetched_from:         https://argovis.colorado.edu\n",
       "    Fetched_by:           gmaze\n",
       "    Fetched_date:         2020/08/26\n",
       "    Fetched_constraints:  phy_[x=-60.00/0.00; y=20.00/60.07; z=0.0/500.0; t=2...\n",
       "    Fetched_uri:          ['https://argovis.colorado.edu/selection/profiles?s..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "ArgoDataFetcher(src='argovis', parallel=False).region(this_box()).to_xarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Required argument 'string' (pos 1) not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mServerDisconnectedError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m~/git/github/euroargodev/argopy/argopy/stores/filesystems.py\u001b[0m in \u001b[0;36mopen_mfjson\u001b[0;34m(self, urls, max_workers, method, progress, preprocess, *args, **kwargs)\u001b[0m\n\u001b[1;32m    628\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m                         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/obidam36/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/obidam36/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/obidam36/lib/python3.6/concurrent/futures/thread.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/github/euroargodev/argopy/argopy/stores/filesystems.py\u001b[0m in \u001b[0;36m_mfprocessor_json\u001b[0;34m(self, url, preprocess, *args, **kwargs)\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0;31m# Load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m         \u001b[0;31m# Pre-process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/github/euroargodev/argopy/argopy/stores/filesystems.py\u001b[0m in \u001b[0;36mopen_json\u001b[0;34m(self, url, **kwargs)\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mof\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m                 \u001b[0mjs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mof\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/obidam36/lib/python3.6/json/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    295\u001b[0m     \"\"\"\n\u001b[0;32m--> 296\u001b[0;31m     return loads(fp.read(),\n\u001b[0m\u001b[1;32m    297\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/obidam36/lib/python3.6/site-packages/fsspec/implementations/http.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, length)\u001b[0m\n\u001b[1;32m    324\u001b[0m         ):\n\u001b[0;32m--> 325\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/obidam36/lib/python3.6/site-packages/fsspec/asyn.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_sync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/obidam36/lib/python3.6/site-packages/fsspec/asyn.py\u001b[0m in \u001b[0;36mmaybe_sync\u001b[0;34m(func, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;31m# run the awaitable on the loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/obidam36/lib/python3.6/site-packages/fsspec/asyn.py\u001b[0m in \u001b[0;36msync\u001b[0;34m(loop, func, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/obidam36/lib/python3.6/site-packages/fsspec/asyn.py\u001b[0m in \u001b[0;36mf\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0mfuture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mawait\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/obidam36/lib/python3.6/site-packages/fsspec/implementations/http.py\u001b[0m in \u001b[0;36masync_fetch_all\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAllBytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m             \u001b[0masync\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/obidam36/lib/python3.6/site-packages/aiohttp/client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, method, str_or_url, params, data, json, cookies, headers, skip_auto_headers, auth, allow_redirects, max_redirects, compress, chunked, expect100, raise_for_status, read_until_eof, proxy, proxy_auth, timeout, verify_ssl, fingerprint, ssl_context, ssl, proxy_headers, trace_request_ctx)\u001b[0m\n\u001b[1;32m    503\u001b[0m                             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m                                 \u001b[0mawait\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m                             \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/obidam36/lib/python3.6/site-packages/aiohttp/client_reqrep.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, connection)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m                     \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpayload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_protocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore  # noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mhttp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHttpProcessingError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/obidam36/lib/python3.6/site-packages/aiohttp/streams.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    590\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m                 \u001b[0mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_waiter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    592\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCancelledError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeoutError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mServerDisconnectedError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/git/github/euroargodev/argopy/argopy/fetchers.py\u001b[0m in \u001b[0;36mto_xarray\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m                 \u001b[0;34m%\u001b[0m \u001b[0;34m\",\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFetchers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m             )\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[0mxds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_xarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m         \u001b[0mxds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostproccessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mxds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/github/euroargodev/argopy/argopy/data_fetchers/argovis_data.py\u001b[0m in \u001b[0;36mto_xarray\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mto_xarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;34m\"\"\" Download and return data as xarray Datasets \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_xarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m         ds = ds.sortby(\n\u001b[1;32m    196\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0;34m\"TIME\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"PRES\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/github/euroargodev/argopy/argopy/data_fetchers/argovis_data.py\u001b[0m in \u001b[0;36mto_dataframe\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         df_list = self.fs.open_mfjson(\n\u001b[0;32m--> 178\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson2dataframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         )\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/github/euroargodev/argopy/argopy/stores/filesystems.py\u001b[0m in \u001b[0;36mopen_mfjson\u001b[0;34m(self, urls, max_workers, method, progress, preprocess, *args, **kwargs)\u001b[0m\n\u001b[1;32m    629\u001b[0m                         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m                         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m                         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m                     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Required argument 'string' (pos 1) not found"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ArgoDataFetcher(src='argovis', parallel=True).region(this_box()).to_xarray();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple comparison shows that parallel request is significantly faster than the standard one."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    ".. warning::\n",
    "\n",
    "    Parallelizing your fetcher is usefull to handle large region of data, but it can also add a significant overhead on *reasonable* size requests that may lead to degraded performances. So, we do not recommand for you to use the parallel option systematically."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
