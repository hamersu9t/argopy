{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "# HIDDEN CELL\n",
    "import sys, os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Importing argopy in dev mode:\n",
    "on_rtd = os.environ.get('READTHEDOCS', None) == 'True'\n",
    "if not on_rtd:\n",
    "    sys.path.insert(0, \"/Users/gmaze/git/github/euroargodev/argopy\")\n",
    "    import git\n",
    "    import argopy\n",
    "    from argopy.options import OPTIONS\n",
    "    print(\"argopy:\", argopy.__version__, \n",
    "          \"\\nsrc:\", argopy.__file__, \n",
    "          \"\\nbranch:\", git.Repo(search_parent_directories=True).active_branch.name, \n",
    "          \"\\noptions:\", OPTIONS)\n",
    "else:\n",
    "    sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "import xarray as xr\n",
    "# xr.set_options(display_style=\"html\");\n",
    "xr.set_options(display_style=\"text\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argopy\n",
    "from argopy import DataFetcher as ArgoDataFetcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve ``argopy`` data fetching performances (in terms of time of retrieval), 2 solutions are available:\n",
    "    \n",
    "- Cache fetched data, i.e. save your request locally so that you don't have to fetch it again,\n",
    "- Fetch data by chunks in parallel, i.e. fetch peace of independant data simultaneously.\n",
    "\n",
    "These solutions are explained below.\n",
    "\n",
    "Note that another solution from standard big data strategies would be to fetch data lazily. But since (i) *argopy* post-processes raw Argo data on the client side and (ii) none of the data sources are cloud/lazy compatible, this solution is not possible (yet)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "If you want to avoid retrieving the same data several times during a working session, or if you fetched a large amount of data, you may want to temporarily save data in a cache file.\n",
    "\n",
    "You can cache fetched data with the fetchers option ``cache``.\n",
    "\n",
    "**Argopy** cached data are persistent, meaning that they are stored locally on files and will survive execution of your script with a new session. \n",
    "**Cached data have an expiration time of one day**, since this is the update frequency of most data sources. This will ensure you always have the last version of Argo data.\n",
    "\n",
    "All data and meta-data (index) fetchers have a caching system.\n",
    "\n",
    "The argopy default cache folder is under your home directory at ``~/.cache/argopy``. \n",
    "\n",
    "But you can specify the path you want to use in several ways:\n",
    "\n",
    "- with **argopy** global options:\n",
    "\n",
    "```python\n",
    "argopy.set_options(cachedir='mycache_folder')\n",
    "```\n",
    "\n",
    "- in a temporary context:\n",
    "\n",
    "```python\n",
    "with argopy.set_options(cachedir='mycache_folder'):\n",
    "    ds = ArgoDataFetcher(cache=True).profile(6902746, 34).to_xarray()\n",
    "```\n",
    "\n",
    "- when instantiating the data fetcher:\n",
    "\n",
    "```python\n",
    "ds = ArgoDataFetcher(cache=True, cachedir='mycache_folder').profile(6902746, 34).to_xarray()\n",
    "```"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    ".. warning::\n",
    "\n",
    "  You really need to set the ``cache`` option to ``True``. Specifying only the ``cachedir`` won't trigger caching !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clearing the cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to manually clear your cache folder, and/or make sure your data are newly fetched, you can do it at the fetcher level with the ``clear_cache`` method.\n",
    "\n",
    "Start to fetch data and store them in cache:\n",
    "\n",
    "```python\n",
    "fetcher = ArgoDataFetcher(cache=True, cachedir='mycache_folder').profile(6902746, 34)\n",
    "fetcher.to_xarray();\n",
    "```\n",
    "\n",
    "Fetched data are in the local cache folder:\n",
    "```python\n",
    "os.listdir('mycache_folder')\n",
    "```\n",
    "```bash\n",
    "['cache', \n",
    " 'c5c820b6aff7b2ef86ef00626782587a95d37edc54120a63ee4699be2b0c6b7c']\n",
    "```\n",
    "\n",
    "where we see one hash entries the newly fetched data and the cache registry file ``cache``.\n",
    "\n",
    "We can then fetch something else using the same cache folder:\n",
    "\n",
    "```python\n",
    "fetcher2 = ArgoDataFetcher(cache=True, cachedir='mycache_folder').profile(1901393, 1)\n",
    "fetcher2.to_xarray();\n",
    "```\n",
    "\n",
    "All fetched data are cached:\n",
    "\n",
    "```python\n",
    "os.listdir('mycache_folder')\n",
    "```\n",
    "```bash\n",
    "['cache',\n",
    " 'c5c820b6aff7b2ef86ef00626782587a95d37edc54120a63ee4699be2b0c6b7c',\n",
    " '58072df8477157c194449a2e6dff8d69ca3c8fded01eebdd8a5fc446f2f7f9a7']\n",
    "```\n",
    "\n",
    "Note the new hash file with the ``fetcher2`` data.\n",
    "\n",
    "It is important to note that we can safely clear the cache from the first ``fetcher`` data, it won't remove the ``fetcher2`` data:\n",
    "\n",
    "```python\n",
    "fetcher.clear_cache()\n",
    "os.listdir('mycache_folder')\n",
    "```\n",
    "```bash\n",
    "['cache', \n",
    " '58072df8477157c194449a2e6dff8d69ca3c8fded01eebdd8a5fc446f2f7f9a7']\n",
    "```\n",
    "\n",
    "By using the fetcher level clear cache, you make sure that only data fetched with it are removed, while other fetched data (with other fetchers for instance) will stay in place.\n",
    "\n",
    "If you want to clear the entire cache folder, whatever the fetcher used, do it at the package level with:\n",
    "\n",
    "```python\n",
    "argopy.clear_cache()\n",
    "```\n",
    "\n",
    "So, if we now check the cache folder, it's been deleted:\n",
    "\n",
    "```python\n",
    "os.listdir('mycache_folder')\n",
    "```\n",
    "```bash\n",
    "---------------------------------------------------------------------------\n",
    "FileNotFoundError                         Traceback (most recent call last)\n",
    "<ipython-input-13-6726e674f21f> in <module>\n",
    "----> 1 os.listdir('mycache_folder')\n",
    "\n",
    "FileNotFoundError: [Errno 2] No such file or directory: 'mycache_folder'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel data fetching\n",
    "\n",
    "Sometimes you may find that your request takes a long time to fetch, or simply does not even succeed. This is probably because you're trying to fetch a large amount of data.\n",
    "\n",
    "In this case, you can try to let argopy chunks your request into smaller pieces and have them fetched in parallel for you. This is done with the argument ``parallel`` of the data fetcher and can be tuned using options ``chunks`` and ``chunksize``.\n",
    "\n",
    "This goes by default like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a box to load (large enough to trigger chunking):\n",
    "box = [-60, -30, 40.0, 60.0, 0.0, 100.0, \"2007-01-01\", \"2007-04-01\"]\n",
    "\n",
    "# Instantiate a parallel fetcher:\n",
    "loader_par = ArgoDataFetcher(src='erddap', parallel=True).region(box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can also use the option ``progress`` to display a progress bar during fetching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_par = ArgoDataFetcher(src='erddap', parallel=True, progress=True).region(box)\n",
    "loader_par"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you can fetch data as usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds = loader_par.to_xarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check how many chunks your request has been split into, you can look at the ``uri`` property of the fetcher, it gives you the list of paths toward data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display only the relevant part of each URLs of URI:\n",
    "for uri in loader_par.uri:\n",
    "    print(\"http: ... \", \"&\".join(uri.split(\"&\")[1:-2])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To control chunking, you can use the **``chunks``** option that specifies the number of chunks in each of the *direction*:\n",
    "\n",
    "- ``lon``, ``lat``, ``dpt`` and ``time`` for a **region** fetching,\n",
    "- ``wmo`` for a **float** and **profile** fetching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a large box:\n",
    "box = [-60, 0, 0.0, 60.0, 0.0, 500.0, \"2007\", \"2010\"]\n",
    "\n",
    "# Init a parallel fetcher:\n",
    "loader_par = ArgoDataFetcher(src='erddap', \n",
    "                             parallel=True, \n",
    "                             chunks={'lon': 5}).region(box)\n",
    "# Check number of chunks:\n",
    "len(loader_par.uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates 195 chunks, and 5 along the longitudinale direction, as requested. \n",
    "\n",
    "When the ``chunks`` option is not specified for a given *direction*, it relies on auto-chunking using pre-defined chunk maximum sizes (see below). \n",
    "In the case above, this explains why we have 195 and not only 5 chunks.\n",
    "\n",
    "To chunk the request along a single direction, set explicitely all the others direction to ``1``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init a parallel fetcher:\n",
    "loader_par = ArgoDataFetcher(src='erddap', \n",
    "                             parallel=True, \n",
    "                             chunks={'lon': 5, 'lat':1, 'dpt':1, 'time':1}).region(box)\n",
    "\n",
    "# Check number of chunks:\n",
    "len(loader_par.uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have 5 chunks along longitude, check out the URLs parameter in the list of URIs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for uri in loader_par.uri:\n",
    "    print(\"&\".join(uri.split(\"&\")[1:-2])) # Display only the relevant URL part"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    ".. note::\n",
    "    You may notice that if you run the last command with the `argovis` fetcher, you will still have more than 5 chunks (i.e. 65). This is because `argovis` is limited to 3 months length requests. So, for this requet that is 3 years long, argopy ends up with 13 chunks along time, times 5 chunks in longitude, leading to 65 chunks in total."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    ".. warning::\n",
    "    The `localftp` fetcher and the `float` and `profile` access points of the `argovis` fetcher use a list of resources than are not chunked but fetched in parallel using a batch queue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Size of chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default chunk size for each access point dimensions are:\n",
    "\n",
    "| Access point dimension | Maximum chunk size |\n",
    "|------------------------|:------------------:|\n",
    "| region / **lon**       |       20 deg       |\n",
    "| region / **lat**       |       20 deg       |\n",
    "| region / **dpt**       |      500 m or db   |\n",
    "| region / **time**      |       90 days      |\n",
    "| float / **wmo**        |          5         |\n",
    "| profile / **wmo**      |          5         |\n",
    "\n",
    "These default values are used to chunk data where the ``chunks`` parameter key is set to ``auto``.\n",
    "\n",
    "But you can modify the maximum chunk size allowed in each of the possible directions. This is done with the option **``chunks_maxsize``**.\n",
    "\n",
    "For instance if you want to make sure that your chunks are not larger then 100 meters (db) in depth (pressure), you can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a large box:\n",
    "box = [-60, -10, 40.0, 60.0, 0.0, 500.0, \"2007\", \"2010\"]\n",
    "\n",
    "# Init a parallel fetcher:\n",
    "loader_par = ArgoDataFetcher(src='erddap', \n",
    "                             parallel=True, \n",
    "                             chunks_maxsize={'dpt': 100}).region(box)\n",
    "# Check number of chunks:\n",
    "len(loader_par.uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this creates a large number of chunks, let's do this again and combine with the option ``chunks`` to see easily what's going on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init a parallel fetcher with chunking along the vertical axis alone:\n",
    "loader_par = ArgoDataFetcher(src='erddap', \n",
    "                             parallel=True, \n",
    "                             chunks_maxsize={'dpt': 100},\n",
    "                             chunks={'lon':1, 'lat':1, 'dpt':'auto', 'time':1}).region(box)\n",
    "\n",
    "for uri in loader_par.uri:\n",
    "    print(\"http: ... \", \"&\".join(uri.split(\"&\")[1:-2])) # Display only the relevant URL part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see, that the ``pres`` argument of this erddap list of URLs define layers not thicker than the requested 100db."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the ``profile`` and ``float`` access points, you can use the ``wmo`` keyword to control the number of WMOs in each chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WMO_list = [6902766, 6902772, 6902914, 6902746, 6902916, 6902915, 6902757, 6902771]\n",
    "\n",
    "# Init a parallel fetcher with chunking along the list of WMOs:\n",
    "loader_par = ArgoDataFetcher(src='erddap', \n",
    "                             parallel=True, \n",
    "                             chunks_maxsize={'wmo': 3}).float(WMO_list)\n",
    "\n",
    "for uri in loader_par.uri:\n",
    "    print(\"http: ... \", \"&\".join(uri.split(\"&\")[1:-2])) # Display only the relevant URL part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see here, that this request for 8 floats is split in chunks with no more that 3 floats each."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    ".. note::\n",
    "    At this point, there is no mechanism to chunk requests along cycle numbers for the ``profile`` access point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelization methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are 3 methods available to set-up your data fetching requests in parallel:\n",
    "    \n",
    "1. [Multi-threading](https://en.wikipedia.org/wiki/Multithreading_(computer_architecture))\n",
    "1. [Multi-processing](https://en.wikipedia.org/wiki/Multiprocessing)\n",
    "1. a [Dask distributed client](https://distributed.dask.org/en/latest/client.html)\n",
    "\n",
    "The first two options use a pool of [threads](https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ThreadPoolExecutor) or [processes](https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ProcessPoolExecutor) managed with the [concurrent futures module](https://docs.python.org/3/library/concurrent.futures.html#module-concurrent.futures). The last option is a much higher level method to leverage the computing power of a Dask distributed framework ([see here for examples on how to create a dask cluster](https://nbviewer.jupyter.org/github/obidam/ds2-2020/blob/master/practice/environment/01-Launch_Dask_Cluster.ipynb)). \n",
    "\n",
    "The parallelization method is set with the ``parallel_method`` option of the fetcher, which can take as values the strings ``thread`` or ``process`` or a *client* object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "box = [-60, 0, \n",
    "       20.0, 60.0 + np.random.randint(0,100,1)[0]/1000, \n",
    "       0.0, 500.0, \n",
    "       \"2007\", \"2010\"]\n",
    "box = [-60, -30, 40.0, 60.0, 0.0, 100.0, \"2007-01-01\", \"2007-04-01\"]\n",
    "\n",
    "print(\"BOX=\", box)\n",
    "ds = ArgoDataFetcher(src='argovis', parallel=True, progress=True, parallel_method='thread').region(box).to_xarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of performances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that to compare performances with or without the parallel option, we need to make sure that data are not cached on the server side.\n",
    "To do this, we use a very small random perturbation on the box definition, here on the maximum latitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def this_box():\n",
    "    return [-60, 0, \n",
    "           20.0, 60.0 + np.random.randint(0,100,1)[0]/1000, \n",
    "           0.0, 500.0, \n",
    "           \"2007\", \"2010\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds = ArgoDataFetcher(src='argovis', parallel=False).region(this_box()).to_xarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds = ArgoDataFetcher(src='argovis', parallel=True).region(this_box()).to_xarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple comparison shows that parallel request is significantly faster than the standard one."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    ".. warning::\n",
    "\n",
    "    Parallelizing your fetcher is usefull to handle large region of data, but it can also add a significant overhead on *reasonable* size requests that may lead to degraded performances. So, we do not recommand for you to use the parallel option systematically."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
